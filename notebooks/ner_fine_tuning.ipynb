{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets seqeval accelerate\n",
    "\n",
    "from google.colab import files\n",
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "file_id = 'file_id'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "\n",
    "# Download the file\n",
    "output = 'labeled_data.conll'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "zip_path = '/content/labeled_data.conll'\n",
    "extract_dir = '/content/extracted_data'\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Extract all files\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extracted files:\", os.listdir(extract_dir))\n",
    "\n",
    "labeled_data = os.path.join(extract_dir, 'labeled_data.conll')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf93df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from seqeval.metrics import classification_report\n",
    "import os\n",
    "\n",
    "# 1. Load CoNLL data\n",
    "def load_conll_file(path):\n",
    "    sentences, labels = [], []\n",
    "    current_tokens, current_tags = [], []\n",
    "    all_tags = set()\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if current_tokens:\n",
    "                    sentences.append(current_tokens)\n",
    "                    labels.append(current_tags)\n",
    "                    current_tokens, current_tags = [], []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    token, tag = splits[0], splits[-1]\n",
    "                    current_tokens.append(token)\n",
    "                    current_tags.append(tag)\n",
    "                    all_tags.add(tag)\n",
    "\n",
    "    # Build vocab\n",
    "    tag_list = sorted(all_tags)\n",
    "    label2id = {label: i for i, label in enumerate(tag_list)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    # Convert string tags to ids\n",
    "    label_list = [[label2id[tag] for tag in seq] for seq in labels]\n",
    "\n",
    "    # Create Dataset\n",
    "    data = Dataset.from_dict({\"tokens\": sentences, \"ner_tags\": label_list})\n",
    "    return data, label2id, id2label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Split & encode labels\n",
    "\n",
    "model_name = \"rasyosef/bert-tiny-amharic\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "\n",
    "# 3. Tokenize\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"], is_split_into_words=True,\n",
    "        padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )\n",
    "\n",
    "# 5. Setup trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/10Academy/NER_Models/Tiny-BERT-ner\",\n",
    "    run_name=\"Tiny-BERT-ner-run1\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # good on T4\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = preds.argmax(-1)\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_preds = [\n",
    "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(preds, labels)\n",
    "    ]\n",
    "\n",
    "    # Avoid crashing on missing metrics\n",
    "    report = classification_report(true_labels, true_preds, output_dict=True, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": report.get(\"accuracy\", 0.0),\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/content/ethioner-tiny-bert-amh\")\n",
    "tokenizer.save_pretrained(\"/content/ethioner-tiny-bert-amh\")\n",
    "save_path = \"/content/drive/MyDrive/10Academy/NER_Models/Tiny-BERT-ner\"\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
